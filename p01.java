package src;

import java.io.IOException;
import java.io.StringReader;
import org.apache.lucene.analysis.CharArraySet;
import org.apache.lucene.analysis.StopFilter;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.core.WhitespaceTokenizer;
import org.apache.lucene.analysis.standard.StandardTokenizer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.custom.CustomAnalyzer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;


class p01{
    public static void main (String[] args) throws IOException{
        String inputString = "Today is sunny. She is a sunny girl. To be or not to be. She is in Berlin today. Sunny Berlin! Berlin is always exciting!";
        standardTokenGeneration(inputString);
        System.out.println();
        whitespaceTokenGeneration(inputString);
        System.out.println();
        stopWordFilter(inputString, false);
        stopWordFilter(inputString, true);
        System.out.println();
        analyser(inputString);
   }

   public static void standardTokenGeneration(String inputString ) throws IOException{
        StandardTokenizer standardTokenizer = new StandardTokenizer();
        standardTokenizer.setReader(new StringReader(inputString));
        TokenStream tokenStream = standardTokenizer;
        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);
        standardTokenizer.reset();
        System.out.println("Tokens generated by Standard Tokenizer: ");
        while (tokenStream.incrementToken()) {
            System.out.println(charTermAttribute.toString());
        }
        standardTokenizer.close();
   }

   public static void whitespaceTokenGeneration(String inputString) throws IOException{
    WhitespaceTokenizer whitespaceTokenizer = new WhitespaceTokenizer();
    whitespaceTokenizer.setReader(new StringReader(inputString));
    TokenStream tokenStream = whitespaceTokenizer;
    CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);
    whitespaceTokenizer.reset();
    System.out.println("Tokens generated by Whitespace Tokenizer: ");
    while (tokenStream.incrementToken()) {
        System.out.println(charTermAttribute.toString());
    }
    whitespaceTokenizer.close();
   }

   public static void stopWordFilter(String inputString, Boolean caseSensitive) throws IOException{
    List<String> result = new ArrayList<String>();
    CharArraySet stopWords=new CharArraySet(5, caseSensitive);
	stopWords.add("was");
	stopWords.add("is");
	stopWords.add("in");
	stopWords.add("to");
	stopWords.add("be");		
	try (StandardTokenizer stream= new StandardTokenizer()) {
		stream.setReader(new StringReader(inputString));
		StopFilter stream2= new StopFilter(stream, stopWords);
		stream2.reset();
		while(stream2.incrementToken()) {
		    result.add(stream2.getAttribute(CharTermAttribute.class).toString());
		}stream2.close();
		} catch (IOException e) {
		    new RuntimeException(e);  
		}
    if(caseSensitive == true){
        System.out.println("Case sensitive: ");
    }
    else{
        System.out.println("Case insensitive: ");
    }
    System.out.println(result);
   }

   public static void analyser(String inputString) throws IOException{
        CustomAnalyzer customs  = CustomAnalyzer.builder(Paths.get("./src"))
        .withTokenizer("standard")
        .addTokenFilter("lowercase")
        .addTokenFilter("porterstem")
        .addTokenFilter("stop", "ignoreCase", "false", "words", "stopwordlist.txt", "format", "wordset")
        .build();

        TokenStream tokenizer4 = customs.tokenStream("", new StringReader(inputString)); 
        CharTermAttribute token = tokenizer4.addAttribute(CharTermAttribute.class);
        tokenizer4.reset();
        System.out.println("Tokens from custom analyser");
        while(tokenizer4.incrementToken()) {
            System.out.println(token);
        }
   }
}